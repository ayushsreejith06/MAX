# ============================
# MAX â€” ENVIRONMENT VARIABLES
# ============================

# --- LLM Configuration (LM Studio Local Server) ---
USE_LLM=true
LLM_BASE_URL=http://127.0.0.1:1234/v1
LLM_MODEL_NAME=llama-3.2-3b-instruct
LLM_API_KEY=

# ============================
# Common project ignores
# (These are reference comments only;
# actual ignores belong in .gitignore.)
# ============================

# Dependencies
node_modules/
/.pnp
.pnp.js

# Testing
/coverage

# Next.js / Build outputs
/.next/
/out/
build
dist

# Production
/build

# Misc
.DS_Store
*.pem

# Debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Local env files
.env*.local

# Vercel
.vercel

# TypeScript
*.tsbuildinfo
next-env.d.ts

# Hardhat/Foundry
cache/
artifacts/
typechain-types/
broadcast/
out/
forge-cache/
forge-out/

# Hardhat
.hardhat/

# Foundry
foundry.toml.local

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# OS
Thumbs.db

# Tauri desktop packaging
src-tauri/target/
src-tauri/Cargo.lock
frontend/out/
frontend-out/

# Desktop build artifacts
*.exe
*.msi
*.dmg
*.AppImage
*.deb
*.rpm
*.pkg
*.tar.gz
*.zip
